{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223fe21d-b1fa-445b-a647-7a7f69a16b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###csv file load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c766b1-d863-4a12-b549-07fe29e7b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\anafa\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\anafa\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anafa\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anafa\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anafa\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anafa\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b5f5d0-0187-4ad8-825b-0b127f3ac108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f348f5ca-b0de-493a-858e-ce83998fe414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                              title  \\\n",
       "0  business  001.txt  Ad sales boost Time Warner profit   \n",
       "1  business  002.txt   Dollar gains on Greenspan speech   \n",
       "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  High fuel prices hit BA's profits   \n",
       "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                             content  \n",
       "0   Quarterly profits at US media giant TimeWarne...  \n",
       "1   The dollar has hit its highest level against ...  \n",
       "2   The owners of embattled Russian oil giant Yuk...  \n",
       "3   British Airways has blamed high fuel prices f...  \n",
       "4   Shares in UK drinks and food firm Allied Dome...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"C:/Users/anafa/OneDrive/Desktop/mainprojectdataset/bbc-news-data.csv\", delimiter='\\t')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9848d2c7-310e-48be-9317-9c73f50dda16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   category                                  combined_features\n",
      "0  business  business quarterly profit u medium giant timew...\n",
      "1  business  business dollar hit highest level euro almost ...\n",
      "2  business  business owner embattled russian oil giant yuk...\n",
      "3  business  business british airway blamed high fuel price...\n",
      "4  business  business share uk drink food firm allied domec...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data if not already done\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'C:/Users/anafa/OneDrive/Desktop/mainprojectdataset/bbc-news-data.csv'\n",
    "articles = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "articles['content'] = articles['content'].fillna('')\n",
    "articles['category'] = articles['category'].fillna('Unknown')\n",
    "\n",
    "# Step 3: Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A) \n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to the 'content' column\n",
    "articles['cleaned_content'] = articles['content'].apply(clean_text)\n",
    "\n",
    "# Step 4: Tokenization and Lemmatization\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply tokenization and lemmatization\n",
    "articles['processed_content'] = articles['cleaned_content'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Step 5: Combine category and processed content for further analysis\n",
    "articles['combined_features'] = articles['category'] + ' ' + articles['processed_content']\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "preprocessed_file_path = 'preprocessed_bbc_news_data.csv'\n",
    "articles.to_csv(preprocessed_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "print(articles[['category', 'combined_features']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab324e7f-9e36-44b2-9269-c77cc57af76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Call centre users 'lose patience'':\n",
      "                                    title  category\n",
      "316    Christmas shoppers flock to tills  business\n",
      "508      Euro firms miss out on optimism  business\n",
      "2011    Call for action on internet scam      tech\n",
      "2057  Mobile multimedia slow to catch on      tech\n",
      "2212  Mobile multimedia slow to catch on      tech\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Load the preprocessed dataset\n",
    "file_path = 'C:/Users/anafa/OneDrive/Desktop/mainprojectdataset/preprocessed_bbc_news_data.csv'  # Path to the preprocessed dataset\n",
    "articles = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Use TF-IDF to vectorize the 'combined_features' column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Convert the combined features to a TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(articles['combined_features'])\n",
    "\n",
    "# Step 3: Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Step 4: Function to get recommendations based on content\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the article that matches the title\n",
    "    idx = articles[articles['title'] == title].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all articles with that article\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the articles based on similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the 5 most similar articles (excluding the input article)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "\n",
    "    # Get the article indices\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the titles and categories of the most similar articles\n",
    "    return articles[['title', 'category']].iloc[article_indices]\n",
    "\n",
    "# Example usage:\n",
    "article_title = \"Call centre users 'lose patience'\"  # Replace with the title of an article from your dataset\n",
    "recommendations = get_recommendations(article_title)\n",
    "print(f\"Recommendations for '{article_title}':\\n\", recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb80db-91e0-4816-a8d3-518144432ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
